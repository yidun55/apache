a2.sources = r1
al.sinks = k1
a2.channels = c1



a2.sources.r1.type = avro
a2.sources.r1.channels = c1
a2.sources.r1.bind = 0.0.0.0
a2.sources.r1.port = 4401
a2.sources.r1.interceptors = i1
a2.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.TimestampInterceptor$Builder

a2.channels.c1.checkpointDir=/home/dyh/test/checkpointDir
a2.channels.c1.dataDirs=/home/dyh/test/dataDirs
a2.channels.c1.type=file
a2.channels.c1.capacity = 100000000
a2.channels.c1.transactionCapacity = 1000000
a2.channels.c1.maxFileSize=214643507100


a2.sinks.k1.type = hdfs
a2.sinks.k1.channel=c1
#a2.sinks.k1.hdfs.path = hdfs://hadoop03:8020/flume
a2.sinks.k1.hdfs.path = /flume/%y-%m-%d/%H-%M
a2.sinks.k1.hdfs.fileType=CompressedStream
a2.sinks.k1.hdfs.codeC=gzip
a2.sinks.k1.hdfs.filePrefix=events-%Y-%m-%d-%H-%M
a2.sinks.k1.hdfs.round = true
a2.sinks.k1.hdfs.roundValue=1
a2.sinks.k1.hdfs.roundUnit = second

a2.sinks.k1.hdfs.rollCount = 0
a2.sinks.k1.hdfs.rollSize = 134217728
#a2.sinks.k1.hdfs.rollInterval=60              #一小时切分一个文件
a2.sinks.k1.hdfs.writeFormat = TEXT
a2.sinks.k1.hdfs.useLocalTimeStamp=true