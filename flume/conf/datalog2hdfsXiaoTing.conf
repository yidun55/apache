#为晓停姐配置监听datalog日志并也入hdfs的flume  hadoop21

datalog4xt.sources = r1
datalog4xt.channels = c1
datalog4xt.sinks = k1

datalog4xt.sources.r1.type=exec
datalog4xt.sources.r1.command=tail -F /home/hdfs/sqoop_import/datalogs/srcdb_tohive_datalogs
datalog4xt.sources.r1.channels=c1
datalog4xt.sources.r1.inputCharset=UTF-8
datalog4xt.sources.r1.deserializer.maxLineLength=304800000
datalog4xt.sources.r1.deserializer.outputCharset=UTF-8
datalog4xt.sources.r1.deserializer=LINE
datalog4xt.sources.r1.decodeErrorPolicy=IGNORE
datalog4xt.sources.r1.batchSize=1


datalog4xt.channels.c1.checkpointDir=/home/hdfs/sqoop_import/datalogs/checkpointDir
datalog4xt.channels.c1.dataDirs=/home/hdfs/sqoop_import/datalogs/dataDirs
datalog4xt.channels.c1.type=file
datalog4xt.channels.c1.capacity = 100000000
datalog4xt.channels.c1.transactionCapacity = 100
datalog4xt.channels.c1.maxFileSize=214643



datalog4xt.sinks.k1.type = hdfs
datalog4xt.sinks.k1.channel=c1
datalog4xt.sinks.k1.hdfs.fileType=DataStream
datalog4xt.sinks.k1.hdfs.path = /user/hive/sqoop_import_logs/%Y/%m
datalog4xt.sinks.k1.hdfs.filePrefix=srcdb_tohive_datalogs_%Y%m%d
datalog4xt.sinks.k1.hdfs.round = true
datalog4xt.sinks.k1.hdfs.roundValue=1
datalog4xt.sinks.k1.hdfs.roundUnit = second
datalog4xt.sinks.k1.hdfs.rollCount = 0
datalog4xt.sinks.k1.hdfs.rollSize = 0
datalog4xt.sinks.k1.hdfs.rollInterval=0
datalog4xt.sinks.k1.hdfs.useLocalTimeStamp=true
